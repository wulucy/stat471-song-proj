---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

# Linear Model Analysis 

## Ordinary Least Squares

We first employed a linear model to approach our goal of predicting a song’s peak chart position. A quick glance at a histogram of peak chart position showed that the response variable’s distribution exhibited a strong negative skew and would not lend itself well for a linear regression analysis in its current form. To address this issue, we considered a variety of transformations including taking the natural logarithm, squaring the data, and reversing the data such that highest charted songs were represented by 100 instead of 1 and then taking the natural logarithm. As the histogram and Q-Q plot indicate, the last transformation left us with the most normal distribution of our response variable, and we proceeded accordingly.

To generate our model, we used the *regsubsets* package and ran backwards selection on all the predictive variables in the dataset excluding `Year` for interpretability reasons. The model that contained 11 variables minimized $C_p$ of the 26 that we generated. From there we manually performed backwards selection to remove variables until all those remaining were significant at the 0.10 level. After two iterations we were left with a model containing 9 variables: `danceability`, `duration_ms`, `valence`, `artist.pop`, `trap`, `indie`, `country`, `folk` and `funk`. Three of the variables came from our table of Spotify audio qualities while the rest either represented the song’s tagged genre or information about the artist.

As we can see from the summary output, our model produced an $R^2$ of 0.10, indicating that our linear representation explained very little of the variation in the response variable.

It quickly became apparent that our linear model would be an inadequate representation of our response variable, particularly when attempting to predict which songs would be the most popular. Our predicted values on the training dataset ranged from 10.48 to 95.32.  Although over 50 entries in the dataset had peak positions of at least top 10, our fitted model was not able to detect the differences between the best songs and the rest of the dataset. Unsurprisingly, our mean squared error generated from the training data was much higher than desired at 524, suggesting that the model’s estimates missed the true positions by around 23 places on average.  The performance on the testing dataset was only slight worse with an MSE of 543.

## Decision Tree (Regression)

Decision trees provided another approach to modeling a song’s predicted chart position.  We fit regression tree to predict the untransformed Peak.Position variable on our training data, employing all the variables in the dataset. For this segment of analysis, we included the Year variable as a factor so that we could capture the interaction between the year a song was released and the other audio and genre measures.  The tree using all available predictors chose 14 splits on the following nine factors: `country`, `Year`, `artist.pop`, `Season`, `key`, `duration_ms`, `instrumentalness`, `valence`, and `loudness`. The predicted values from the model ranged from 33 to 91 and generated an MSE of 378 on the training data. When applied to the testing data, the MSE rose to 488, a considerable improvement over the least squares’ regression model.

## Random Forest (Regression)

Random Forest was the last method we employed from the family of linear models. To tune our parameters, we analyzed how OOB testing error changed with the number of trees from 0 to 500 with a fixed mtry of 10. As the plot indicates, the error measurements stabilized around ntree = 200. We then analyzed the MSEs of Random Forests generated with mtry values ranging from 1 to 15, ultimately choosing mtry = 10 as it minimized the OOB error. At approximately 234. 

When applied to the training data our model generated an MSE of 42, but it quickly became apparent that our Random Forest was considerably overfitting in light of the testing MSE of 450. Although the performance on the testing data was still underwhelming, it was still an improvement over our other linear methods.

# Classification Model Analysis

The obvious inadequacies of the linear regression models necessitated that we consider new approaches to predict a song’s popularity and chart performance. The category “Top-40” is a commonly used grouping to differentiate the most popular songs from others, even within the Billboard Top 100. Songs that are charted in this echelon are commonly included in popular playlists and garner considerable radio playtime. As such, we saw practical application value in capturing the differences between Top-40 entries and songs that were on the brink of widespread recognition but just fell short. 

One concern with this approach was that there was how to address the severe imbalance between the songs in our dataset that were Top-40 and those that were not. Only 8% of the entries in our training data fell into our target category, which we knew would drastically hamper our models’ classification abilities. To combat this issue, we oversampled the Top-40 songs from our training dataset such that the distribution for our response variable was evenly split between positive and negative indicator values, resulting in 2137 entries for both Top-40 and non-Top-40 songs.  The testing dataset was left untouched.

## Logistic Regression

To begin we fit a logistic regression to predict our dummy variable top40. We then removed the coefficients with the highest p-values until there were only 15 remaining, the maximum number of variables accepted by the bestglm package. The best model found by the exhaustive search contained nine variables, all of which were significant at the 0.05 level. To formulate classification predictions, we employed a threshold of 0.5. Given an even split between positive and negative values in our response variable, we achieved a training MCE of 0.32 from our logistic regression model. Unfortunately, the model performed much worse on our testing dataset, reporting a testing MCE of 0.35 when only 8.8% of the data points were actually in the Top-40. While we could have trivially achieved a much better MCE by predicting negative values for all our data points, our confusion matrix showed that we had a sensitivity 0.70, indicating that were able to classify a considerable portion of the positive values. The false positive rate of 0.82, however, left much to be desired form our model.

## Decision Tree (Classification)

The classification tree fit on all variables generated 16 splits on a subset of 11 variables from 28 made available. Applying our threshold of 0.5 once again, our tree gave us a MCE of 0.22 on the training data and 0.38 on the testing data. Overall, the testing results were much worse for our decision tree when compared to the logistic regression. As can be concluded from the confusion matrix, sensitivity decreased to 0.68 while the false positive rate climbed to 0.86.


## Random Forest (Classification)

The last classification model employed on the audio, genre, artist and release date variables was a Random Forest fit on all variables. The model output indicates that our OOB estimate of error rate was approximately 0.26%, but those results did not translate to the testing data. Our MCE was approximately 0.08. Given that 8.8% of the underlying test data had positive response variables, we would have only slightly beat the MCE from a model that predicted all negative values. Furthermore, the model only correctly identified 12 Top-40 songs, making for a sensitivity rate of 0.14. Needless to say, the Random Forest fit did not provide a robust model of our dataset, but it had the lowest false positive rate of the three models at 0.40.



# Appendix

## Linear Model Diagnostics

```{r, echo=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
df2000_extra <- read.csv('df2000_withgenre_withyear.csv')

# Linear regression - AZ
lm_df <- df2000_extra %>%
  select(-artist, -song, -WeekID, -Weeks.on.Chart, -X.1, -X)

lm_df$Year <- as.factor(lm_df$Year)

summary(lm_df)

# Remove the columns that have no or very few observations
lm_df_1 <- lm_df %>%
  select(-instrumental, -bluegrass, -jazz, -classical, -broadway, -opera, -hip.hop)

# Add transformed variable to new data frame
lm_df_1$Peak.Transformed <- log(101 - lm_df$Peak.Position)
lm_df_1$Peak.Position <- NULL

set.seed(22)
train_indices <- sample(nrow(lm_df_1), nrow(lm_df_1) * 0.7, replace=FALSE)
train_df <- lm_df_1[train_indices,]
test_df <- lm_df_1[-train_indices,]

# Consider transformations
fit.linear <- lm(Peak.Position ~., lm_df)
fit.log <- lm(log(Peak.Position) ~. , lm_df)
fit.reverse <- lm(log(101- Peak.Position) ~., lm_df)
fit.square <- lm(Peak.Position^2 ~. , lm_df)
```

```{r, echo=FALSE, warning=FALSE}
# Normal Q-Q Plots
par(mfrow = c(2,2))
plot(fit.linear, 2, main = "No Transformation")
plot(fit.log, 2, main = "Log Transformation")
plot(fit.reverse, 2, main = "Log+Reverse Transformation")
plot(fit.square, 2, main = "Square Transformation")

# Histograms of transformed distribution
par(mfrow = c(2,2))
hist(lm_df$Peak.Position)
hist(I(log(lm_df$Peak.Position)))
hist(I(log(101 - lm_df$Peak.Position)))
hist(I(lm_df$Peak.Position^2))
```

## Linear Model (OLS) Backwards Selection

```{r, include=FALSE}
# Backwards model selection with regsubsets (don't include Year)
library(leaps)
fit.exh <- regsubsets(Peak.Transformed ~ . -Year, train_df, nvmax=25, method="backward")
f.e <- summary(fit.exh)
data.frame(variables = (1:length(f.e$rsq)),
           r_squared = f.e$rsq,
           rss = f.e$rss,
           bic = f.e$bic,
           cp = f.e$cp)

# Model with 11 variables minimized cp. Remove variables until all are significant
fit.backwards <- lm(Peak.Transformed ~ danceability + duration_ms + instrumentalness + valence +
                      artist.pop + trap + indie + country + folk + rock + funk, train_df)
# fit.backwards.1 <- lm(Peak.Transformed ~ danceability + duration_ms + valence +
#                         artist.pop + trap + indie + country + folk + rock + funk, lm_df_1)
fit.backwards.2 <- lm(Peak.Transformed ~ danceability + duration_ms + valence +
                        artist.pop + trap + country + folk + rock + funk, lm_df_1)
# Our lm final model
fit.final <- fit.backwards.2 
```

```{r, echo=FALSE}
summary(fit.final)
```

## Decision Tree (Regression)

```{r, include=FALSE, warning=FALSE}
# Regression trees
library(rpart)
library(party)
library(rattle)		
library(rpart.plot)	

# Add Peak.Position back in
lm_df_2 <- lm_df_1
lm_df_2$Peak.Position <- lm_df$Peak.Position
lm_df_2$Peak.Transformed <- NULL
train_df_2 <- lm_df_2[train_indices,]
test_df_2 <- lm_df_2[-train_indices,]

tree.full <- rpart(Peak.Position ~ ., train_df_2)
```

```{r, echo=FALSE}
par(mfrow = c(1,1))
fancyRpartPlot(tree.full)
```

## Random Forest (Regression)

```{r, echo=FALSE, warning=FALSE, include=FALSE}
library(randomForest)
fit.rf <- randomForest(Peak.Position ~., train_df_2, mtry=10, ntree=500) 
```

```{r, echo=FALSE, warning=FALSE}
par(mfrow=c(1,1))
plot(fit.rf, col="blue", pch=16, type="p", main="default plot")
```

```{r, echo=FALSE, warning=FALSE}
rf.error.p <- 1:15

for (p in 1:15) {
  fit.rf <- randomForest(Peak.Position ~ ., train_df_2, mtry=p, ntree=200) 
  rf.error.p[p] <- fit.rf$mse[200] 
}

plot(1:15, rf.error.p[1:15], pch=16, xlab="mtry",
     ylab="OOB mse of mtry") 
lines(1:15, rf.error.p[1:15])
```
```{r, include=FALSE, eval=FALSE}
# choose mtry = 10 for final rf
fit.rf.final <- randomForest(Peak.Position ~ ., train_df_2, mtry=10, ntree=200) 

# Testing errors
plot(fit.rf.final$mse, xlab="number of trees", col="blue", ylab="ave mse up to i many trees using OOB predicted", pch=16)
fit.rf.final$mse[200]

yhat.rf.train <- predict(fit.rf.final, train_df_2)
mse.rf.train <- mean((train_df_2$Peak.Position-yhat.rf.train)^2)

yhat.rf.test <- predict(fit.rf.final, test_df_2)
mse.rf.test <- mean((test_df_2$Peak.Position-yhat.rf.test)^2)

hist(yhat.rf.train)
range(yhat.rf.train)
hist(yhat.rf.test)
range(yhat.rf.test)
```

## Logistic Regression
```{r, echo=FALSE, warning=FALSE, include=FALSE}
# Classification data
train_class <- train_df_2
test_class <- test_df_2

train_class$top40 <- as.numeric(train_class$Peak.Position <= 40)
test_class$top40 <- as.numeric(test_class$Peak.Position <= 40)

train_class$Peak.Position <- NULL
test_class$Peak.Position <- NULL
sum(train_class$top40) / nrow(train_class)

# Create oversampled training dataset to account for imbalance between top40 and not top40

top40_index <- which(train_class$top40 == 1)
set.seed(20)
oversampled_index <- sample(top40_index, sum(train_class$top40 == 0) - sum(train_class$top40 == 1), replace=TRUE)
train_class_os <- rbind(train_class, train_class[oversampled_index,])

library(bestglm)

fit.glm.full <- glm(top40 ~ ., train_class_os, family='binomial')

summary(fit.glm.full)

pred.full.train <- as.numeric(predict(fit.glm.full, train_class_os, type='response') >= 0.5)

mce.full.train <- mean(pred.full.train != train_class_os$top40)

pred.full.test <- as.numeric(predict(fit.glm.full, test_class, type='response') >= 0.5)

mce.full.test <- mean(pred.full.test != test_class$top40)

cm.full <- table(pred.full.test, test_class$top40)

cm.full

# Prune down the variables to feed into bestglm

fit.glm.prune <- glm(top40 ~  acousticness + danceability + energy + liveness +
                       loudness + rap + rock + artist.pop + trap + indie + metal + country + folk +
                       funk, train_class_os, family = 'binomial')

summary(fit.glm.prune)
```

```{r, echo=FALSE, warning=FALSE}
Xy <- model.matrix(top40 ~  acousticness + danceability + energy + liveness +
                     loudness + rap + rock + artist.pop + trap + indie + metal + country + folk +
                     funk + 0 , train_class_os) 

Xy <- data.frame(Xy, train_class_os$top40)

#fit.all <- bestglm(Xy, family = binomial, method = "exhaustive", IC="AIC", nvmax = 15)

#load('fit.all.RData')

#fit.all$BestModel

fit.glm <- glm(top40 ~ acousticness + danceability + energy +  rock + liveness + loudness + artist.pop + trap + indie + 
                 metal + country + folk + funk, train_class_os,  family = 'binomial')

summary(fit.glm)

pred.glm.train <- as.numeric(predict(fit.glm, train_class_os, type='response') >= 0.5)

mce.glm.train <- mean(pred.glm.train != train_class_os$top40)

pred.glm.test <- as.numeric(predict(fit.glm, test_class, type='response') >= 0.5)

mce.glm.test <- mean(pred.glm.test != test_class$top40)

cm.glm <- table(pred.glm.test, test_class$top40)
cm.glm

#cm.glm[2,2]/sum(test_class$top40 == "1")
```

## Classification Tree 

```{r, echo=FALSE, warning=FALSE}
tree.class <- rpart(top40 ~ ., train_class_os)
fancyRpartPlot(tree.class)
pred.tree.train <- as.numeric(predict(tree.class, train_class_os) >= 0.5)
mce.tree.train.class <- mean(pred.tree.train != train_class_os$top40)

pred.tree.test <- as.numeric(predict(tree.class, test_class) >= 0.5)
mce.tree.test.class <- mean(pred.tree.test != test_class$top40)
cm.tree <-table(pred.tree.test, test_class$top40)
#cm.tree[2,2]/sum(test_class$top40 == "1")
```

## Random Forest (Classification)
```{r, echo=FALSE, warning=FALSE}
par(mfrow=c(1,1))
set.seed(22)
fit.rf.class <- randomForest(as.factor(top40) ~ ., train_class_os, ntree=200) 
plot(fit.rf.class)

#fit.rf.class

fit.rf.pred.test <- predict(fit.rf.class, test_class, type="response") 
mce.rf.test <- mean(test_class$top40 != fit.rf.pred.test)

cm.rf <- table(fit.rf.pred.test, test_class$top40)
cm.rf
```

